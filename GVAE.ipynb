{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import TensorDataset\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.nn import TransformerConv, global_max_pool, GATv2Conv, ClusterGCNConv, PointTransformerConv, global_mean_pool, dense_mincut_pool\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "import time\n",
    "import torch \n",
    "from torch_geometric.data import Data, InMemoryDataset, download_url\n",
    "import torch_geometric.transforms as T\n",
    "import networkx as nx\n",
    "from torch_cluster import knn_graph\n",
    "from torch_geometric.utils import from_networkx, to_dense_adj, dense_to_sparse, to_dense_batch\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset, download_url\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "class Quark_Gluon_Dataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(Quark_Gluon_Dataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def create_graph(self, image):\n",
    "        G = nx.Graph()\n",
    "        for i in range(image.shape[0]):\n",
    "            for j in range(image.shape[1]):\n",
    "                if image[i][j].any() > 0:\n",
    "                    #add node (i,j) to graph with attribute 'x' = image[i][j] and pos = (i,j)\n",
    "                    G.add_node((i,j), x=image[i][j], pos=(i,j,0))\n",
    "                    if i > 0 and image[i-1][j].any() > 0:\n",
    "                        G.add_edge((i,j), (i-1,j))\n",
    "                    if j > 0 and image[i][j-1].any() > 0:\n",
    "                        G.add_edge((i,j), (i,j-1))\n",
    "                    if i < image.shape[0]-1 and image[i+1][j].any() > 0:\n",
    "                        G.add_edge((i,j), (i+1,j))\n",
    "                    if j < image.shape[1]-1 and image[i][j+1].any() > 0:\n",
    "                        G.add_edge((i,j), (i,j+1))\n",
    "                    if i > 0 and j > 0 and image[i-1][j-1].any() > 0:\n",
    "                        G.add_edge((i,j), (i-1,j-1))\n",
    "                    if i < image.shape[0]-1 and j < image.shape[1]-1 and image[i+1][j+1].any() > 0:\n",
    "                        G.add_edge((i,j), (i+1,j+1))\n",
    "                    if i > 0 and j < image.shape[1]-1 and image[i-1][j+1].any() > 0:\n",
    "                        G.add_edge((i,j), (i-1,j+1))\n",
    "                    if i < image.shape[0]-1 and j > 0 and image[i+1][j-1].any() > 0:\n",
    "                        G.add_edge((i,j), (i+1,j-1))\n",
    "\n",
    "        return G\n",
    "\n",
    "    def process(self):\n",
    "        f = h5py.File('/hdfs1/Data/Shrutimoy/quark-gluon_data-set_n139306.hdf5', 'r')\n",
    "        X_jets = np.asarray(f['X_jets'])\n",
    "        m0 = np.asarray(f['m0'])\n",
    "        pt = np.asarray(f['pt'])\n",
    "        y = np.asarray(f['y'])\n",
    "        data_list = []\n",
    "        for i in tqdm(range(len(X_jets))):\n",
    "            G = self.create_graph(X_jets[i])\n",
    "            data = from_networkx(G)\n",
    "            data.y = torch.tensor(y[i], dtype=torch.long)\n",
    "            data.m = torch.tensor(m0[i], dtype=torch.float)\n",
    "            data.p = torch.tensor(pt[i], dtype=torch.float)\n",
    "            data_list.append(data)\n",
    "\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, test_size):\n",
    "    dataset_size = len(X)\n",
    "    train_data_X = X[:int(dataset_size*(1-test_size))]\n",
    "    test_data_X = X[int(dataset_size*(1-test_size)):]\n",
    "    train_data_y = y[:int(dataset_size*(1-test_size))]\n",
    "    test_data_y = y[int(dataset_size*(1-test_size)):]\n",
    "    return train_data_X, test_data_X, train_data_y, test_data_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function for Graph Autoencoder\n",
    "def loss_function(recon_x, x, mu, logvar, mc1_loss, o1_loss, mc2_loss, o2_loss, mc3_loss, o3_loss):\n",
    "    #print(recon_x.shape, x.shape)\n",
    "    BCE = F.mse_loss(recon_x, x)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD + mc1_loss + o1_loss + mc2_loss + o2_loss + mc3_loss + o3_loss\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.Sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.Sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.Sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph Autoencoder Model\n",
    "class GAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, cluster_dim, dropout, batch_size):\n",
    "        super(GAE, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.fc0 = nn.Linear(input_dim+3, hidden_dim[0])\n",
    "        self.batch_norm0 = nn.BatchNorm1d(hidden_dim[0])\n",
    "        self.conv1 = nn.ModuleList()\n",
    "        self.fc11 = nn.Linear(hidden_dim[0]*2, hidden_dim[1])\n",
    "        self.batch_norm11 = nn.BatchNorm1d(hidden_dim[1])\n",
    "        self.mlp1 = nn.Sequential()\n",
    "        self.conv2 = nn.ModuleList()\n",
    "        self.fc12 = nn.Linear(hidden_dim[1]*2, hidden_dim[2])\n",
    "        self.batch_norm12 = nn.BatchNorm1d(hidden_dim[2])\n",
    "        self.mlp2 = nn.Sequential()\n",
    "        self.conv3 = nn.ModuleList()\n",
    "        self.fc13 = nn.Linear(hidden_dim[2]*2, hidden_dim[3])\n",
    "        self.batch_norm13 = nn.BatchNorm1d(hidden_dim[3])\n",
    "        self.mlp3 = nn.Sequential()\n",
    "        self.conv4 = nn.ModuleList()\n",
    "        self.fc14 = nn.Linear(hidden_dim[3]*2, hidden_dim[4])\n",
    "        self.batch_norm14 = nn.BatchNorm1d(hidden_dim[4])\n",
    "\n",
    "        self.conv1.append(GATv2Conv(hidden_dim[0], hidden_dim[0], add_self_loops=True, dropout=dropout))\n",
    "        self.conv1.append(ClusterGCNConv(hidden_dim[0], hidden_dim[0], add_self_loops=True, dropout=dropout))\n",
    "        self.mlp1.append(nn.Linear(hidden_dim[1], hidden_dim[1]))\n",
    "        self.mlp1.append(nn.ReLU())\n",
    "        self.mlp1.append(nn.Linear(hidden_dim[1], cluster_dim[0]))\n",
    "\n",
    "        self.conv2.append(GATv2Conv(hidden_dim[1], hidden_dim[1], add_self_loops=True, dropout=dropout, edge_dim=1))\n",
    "        self.conv2.append(ClusterGCNConv(hidden_dim[1], hidden_dim[1], add_self_loops=True, dropout=dropout))\n",
    "        self.mlp2.append(nn.Linear(hidden_dim[2], hidden_dim[2]))\n",
    "        self.mlp2.append(nn.ReLU())\n",
    "        self.mlp2.append(nn.Linear(hidden_dim[2], cluster_dim[1]))\n",
    "    \n",
    "        self.conv3.append(GATv2Conv(hidden_dim[2], hidden_dim[2], add_self_loops=True, dropout=dropout, edge_dim=1))\n",
    "        self.conv3.append(ClusterGCNConv(hidden_dim[2], hidden_dim[2], add_self_loops=True, dropout=dropout))\n",
    "        self.mlp3.append(nn.Linear(hidden_dim[3], hidden_dim[3]))\n",
    "        self.mlp3.append(nn.ReLU())\n",
    "        self.mlp3.append(nn.Linear(hidden_dim[3], cluster_dim[2]))\n",
    "\n",
    "        self.conv4.append(GATv2Conv(hidden_dim[3], hidden_dim[3], add_self_loops=True, dropout=dropout, edge_dim=1))\n",
    "        self.conv4.append(ClusterGCNConv(hidden_dim[3], hidden_dim[3], add_self_loops=True, dropout=dropout))\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim[4], hidden_dim[5])\n",
    "        self.fc2 = nn.Linear(hidden_dim[4], hidden_dim[5])\n",
    "        self.fc3 = nn.Linear(hidden_dim[5], hidden_dim[4])\n",
    "\n",
    "        self.conv5 = nn.ModuleList()\n",
    "        self.fc21 = nn.Linear(hidden_dim[4]*2, hidden_dim[3])\n",
    "        self.batch_norm21 = nn.BatchNorm1d(hidden_dim[3])\n",
    "        self.conv6 = nn.ModuleList()\n",
    "        self.fc22 = nn.Linear(hidden_dim[3]*2, hidden_dim[2])\n",
    "        self.batch_norm22 = nn.BatchNorm1d(hidden_dim[2])\n",
    "        self.conv7 = nn.ModuleList()\n",
    "        self.fc23 = nn.Linear(hidden_dim[2]*2, hidden_dim[1])\n",
    "        self.batch_norm23 = nn.BatchNorm1d(hidden_dim[1])\n",
    "        self.conv8 = nn.ModuleList()\n",
    "        self.fc24 = nn.Linear(hidden_dim[1]*2, hidden_dim[0])\n",
    "        self.batch_norm24 = nn.BatchNorm1d(hidden_dim[0])\n",
    "\n",
    "        self.conv5.append(GATv2Conv(hidden_dim[4], hidden_dim[4], add_self_loops=True, dropout=dropout, edge_dim=1))    \n",
    "        self.conv5.append(ClusterGCNConv(hidden_dim[4], hidden_dim[4], add_self_loops=True, dropout=dropout))\n",
    "        \n",
    "\n",
    "        self.conv6.append(GATv2Conv(hidden_dim[3], hidden_dim[3], add_self_loops=True, dropout=dropout, edge_dim=1))\n",
    "        self.conv6.append(ClusterGCNConv(hidden_dim[3], hidden_dim[3], add_self_loops=True, dropout=dropout))\n",
    "        \n",
    "\n",
    "        self.conv7.append(GATv2Conv(hidden_dim[2], hidden_dim[2], add_self_loops=True, dropout=dropout, edge_dim=1))\n",
    "        self.conv7.append(ClusterGCNConv(hidden_dim[2], hidden_dim[2], add_self_loops=True, dropout=dropout))\n",
    "        \n",
    "\n",
    "        self.conv8.append(GATv2Conv(hidden_dim[1], hidden_dim[1], add_self_loops=True, dropout=dropout, edge_dim=1))\n",
    "        self.conv8.append(ClusterGCNConv(hidden_dim[1], hidden_dim[1], add_self_loops=True, dropout=dropout))\n",
    "        \n",
    "\n",
    "        self.fc4 = nn.Linear(hidden_dim[0], input_dim)\n",
    "\n",
    "    def encode(self, x, edge_index, pos, batch):\n",
    "        #print(\"encoder start\")\n",
    "        pos = pos.float()\n",
    "        x = torch.cat((x, pos), dim=1)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        x = self.fc0(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.batch_norm0(x)\n",
    "        #print(\"1st layer\")\n",
    "        x1 = self.conv1[0](x, edge_index)\n",
    "        x3 = self.conv1[1](x, edge_index)\n",
    "        x = torch.cat((x1, x3), dim=1)\n",
    "        x = self.fc11(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.batch_norm11(x)\n",
    "        adj = to_dense_adj(edge_index, max_num_nodes=x.size(0))\n",
    "        x_new, mask = to_dense_batch(x)\n",
    "        s1 = self.mlp1(x_new)\n",
    "        x, adj, mc1_loss, o1_loss = dense_mincut_pool(x_new, adj, s1, mask)\n",
    "        x = x.reshape(x.size(0)*x.size(1), -1)\n",
    "        edge_index, edge_attr = dense_to_sparse(adj)\n",
    "\n",
    "        #print(\"2nd layer\")  \n",
    "        x1 = self.conv2[0](x, edge_index, edge_attr=edge_attr)\n",
    "        x3 = self.conv2[1](x, edge_index)\n",
    "        x = torch.cat((x1, x3), dim=1)\n",
    "        x = self.fc12(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.batch_norm12(x)\n",
    "        adj = to_dense_adj(edge_index, max_num_nodes=x.size(0))\n",
    "        x_new, mask = to_dense_batch(x)\n",
    "        s2 = self.mlp2(x_new)\n",
    "        x, adj, mc2_loss, o2_loss = dense_mincut_pool(x_new, adj, s2, mask)\n",
    "        edge_index, edge_attr = dense_to_sparse(adj)\n",
    "\n",
    "        #print(\"3rd layer\")\n",
    "        x = x.reshape(x.size(0)*x.size(1), -1)\n",
    "        x1 = self.conv3[0](x, edge_index, edge_attr=edge_attr)\n",
    "        x3 = self.conv3[1](x, edge_index)\n",
    "        x = torch.cat((x1, x3), dim=1)\n",
    "        x = self.fc13(x)  \n",
    "        x = F.relu(x)\n",
    "        x = self.batch_norm13(x)\n",
    "        adj = to_dense_adj(edge_index, max_num_nodes=x.size(0))\n",
    "        x_new, mask = to_dense_batch(x)\n",
    "        s3 = self.mlp3(x_new)\n",
    "        x, adj, mc3_loss, o3_loss = dense_mincut_pool(x_new, adj, s3, mask)\n",
    "        edge_index, edge_attr = dense_to_sparse(adj)\n",
    "        last_edge = edge_index\n",
    "        last_attr = edge_attr\n",
    "\n",
    "        #print(\"4th layer\")\n",
    "        x = x.reshape(x.size(0)*x.size(1), -1)\n",
    "        x1 = self.conv4[0](x, edge_index, edge_attr=edge_attr)\n",
    "        x3 = self.conv4[1](x, edge_index)\n",
    "        x = torch.cat((x1, x3), dim=1)\n",
    "        x = self.fc14(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.batch_norm14(x)\n",
    "        #print(\"encoder end\")\n",
    "        return self.fc1(x), self.fc2(x), mc1_loss, o1_loss, mc2_loss, o2_loss, mc3_loss, o3_loss, s1, s2, s3, last_edge, last_attr\n",
    "    \n",
    "    def reparameterize(self, mu, sigma):\n",
    "        #print(\"in reparameterize\")\n",
    "        std = sigma.mul(0.5).exp_()\n",
    "        eps = torch.FloatTensor(std.size()).normal_().to(device)\n",
    "        eps = Variable(eps)\n",
    "        #print(\"reparameterize end\")\n",
    "        return eps.mul(std).add_(mu)\n",
    "    \n",
    "    def decode(self, z, last_edge, last_attr, S1, S2, S3):\n",
    "        #print(\"in decode\")\n",
    "        z = self.fc3(z)\n",
    "        z1 = self.conv5[0](z, last_edge, last_attr)\n",
    "        z2 = self.conv5[1](z, last_edge)\n",
    "        z = torch.cat((z1, z2), dim=1)\n",
    "        z = self.fc21(z)\n",
    "        z = F.relu(z)\n",
    "        z = self.batch_norm21(z)\n",
    "        s = torch.squeeze(S3)\n",
    "        z = torch.matmul(s, z)\n",
    "        adj = to_dense_adj(last_edge, edge_attr=last_attr, max_num_nodes=z.size(0))\n",
    "        #print(adj.shape, S[2].reshape(S[2].shape[1], S[2].shape[2]).shape)\n",
    "        adj_pool = torch.matmul(s.T, adj)\n",
    "        #print(adj_pool.shape)\n",
    "        adj_pool = torch.matmul(adj_pool, s)\n",
    "        edge_index, edge_attr = dense_to_sparse(adj_pool)\n",
    " \n",
    "        z1 = self.conv6[0](z, edge_index, edge_attr)\n",
    "        z2 = self.conv6[1](z, edge_index)\n",
    "        z = torch.cat((z1, z2), dim=1)\n",
    "        z = self.fc22(z)\n",
    "        z = F.relu(z)\n",
    "        z = self.batch_norm22(z)\n",
    "        s = torch.squeeze(S2)\n",
    "        z = torch.matmul(s, z)\n",
    "        adj = to_dense_adj(edge_index, max_num_nodes=z.size(0))\n",
    "        adj_pool = torch.matmul(s.T, adj)\n",
    "        adj_pool = torch.matmul(adj_pool, s)\n",
    "        edge_index, edge_attr = dense_to_sparse(adj_pool)\n",
    "\n",
    "        z1 = self.conv7[0](z,edge_index, edge_attr)\n",
    "        z2 = self.conv7[1](z, edge_index)\n",
    "        z = torch.cat((z1, z2), dim=1)\n",
    "        z = self.fc23(z)\n",
    "        z = F.relu(z)\n",
    "        z = self.batch_norm23(z)\n",
    "        s = torch.squeeze(S1)\n",
    "        z = torch.matmul(s, z)\n",
    "        adj = to_dense_adj(edge_index, max_num_nodes=z.size(0))\n",
    "        adj_pool = torch.matmul(s.T, adj)\n",
    "        adj_pool = torch.matmul(adj_pool, s)\n",
    "        edge_index, edge_attr = dense_to_sparse(adj_pool)\n",
    "\n",
    "        z1 = self.conv8[0](z, edge_index, edge_attr)\n",
    "        z2 = self.conv8[1](z, edge_index)\n",
    "        z = torch.cat((z1, z2), dim=1)\n",
    "        z = self.fc24(z)\n",
    "        z = F.relu(z)\n",
    "        z = self.batch_norm24(z)\n",
    "        \n",
    "        z = self.fc4(z)\n",
    "        #print(z.shape)\n",
    "        #print(\"out decode\")\n",
    "        z = F.softmax(z)\n",
    "        return z\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, pos, batch = data.x, data.edge_index, data.pos, data.batch\n",
    "        mu, sigma, mc1_loss, o1_loss, mc2_loss, o2_loss, mc3_loss, o3_loss, s1, s2, s3, last_edge, last_attr = self.encode(x, edge_index, pos, batch)\n",
    "        z = self.reparameterize(mu, sigma)\n",
    "        #print(S[0].shape, S[1].shape, S[2].shape)\n",
    "        x_new = self.decode(z, last_edge, last_attr, s1, s2, s3)\n",
    "        return x_new, mu, sigma, mc1_loss, o1_loss, mc2_loss, o2_loss, mc3_loss, o3_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Quark_Gluon_Dataset(root='/hdfs1/Data/Shubhajit/Quark_Gluon_Data_1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train, validation and test\n",
    "train_dataset = dataset[:int(0.8*len(dataset))]\n",
    "val_dataset = dataset[int(0.8*len(dataset)):int(0.9*len(dataset))]\n",
    "test_dataset = dataset[int(0.9*len(dataset)):]\n",
    "\n",
    "# load the dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the model\n",
    "model = GAE(3, [16, 32, 64, 128, 256, 512], [256, 128, 64], 0, 16).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6966 [00:00<?, ?it/s]/home/royshubhajit/torch_pip/lib/python3.7/site-packages/ipykernel_launcher.py:205: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  1%|▏         | 100/6966 [00:32<39:32,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 19.2400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 200/6966 [01:06<40:18,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 9.6792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 300/6966 [01:39<38:15,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 6.5009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 400/6966 [02:13<40:34,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 4.8985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 500/6966 [02:46<33:17,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 3.9386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 600/6966 [03:19<31:56,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 3.2969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 700/6966 [03:53<31:31,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 2.8380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 800/6966 [04:26<34:36,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 2.4936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 900/6966 [05:00<32:43,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 2.2256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1000/6966 [05:34<34:48,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 2.0112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1100/6966 [06:07<35:10,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 1.8357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1200/6966 [06:41<32:24,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 1.6894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 1300/6966 [07:15<33:47,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 1.5656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1400/6966 [07:49<32:05,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 1.4595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 1500/6966 [08:22<28:36,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 1.3674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 1600/6966 [08:56<29:21,  3.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 1.2870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 1700/6966 [09:29<27:56,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 1.2160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 1800/6966 [10:02<27:03,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 1.1528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 1900/6966 [10:36<29:17,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 1.0962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2000/6966 [11:09<29:20,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 1.0454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 2100/6966 [11:44<28:28,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.9993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 2200/6966 [12:17<26:23,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.9575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2300/6966 [12:51<26:31,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.9193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 2400/6966 [13:25<25:18,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.8843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 2500/6966 [13:59<26:16,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.8521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 2600/6966 [14:33<24:24,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.8224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 2700/6966 [15:07<24:20,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.7949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2800/6966 [15:41<23:32,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.7695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 2900/6966 [16:13<21:46,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.7457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3000/6966 [16:47<23:32,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.7236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 3100/6966 [17:21<22:51,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.7030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 3200/6966 [17:55<20:42,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.6836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 3300/6966 [18:29<20:25,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.6655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 3400/6966 [19:03<19:52,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.6484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3500/6966 [19:37<19:41,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.6324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 3600/6966 [20:11<18:35,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.6172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 3700/6966 [20:47<19:22,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.6029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 3800/6966 [21:20<16:48,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.5894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 3900/6966 [21:53<16:15,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.5766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 4000/6966 [22:27<15:54,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.5644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 4100/6966 [23:01<16:22,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.5528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 4200/6966 [23:35<15:35,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.5424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 4300/6966 [24:08<14:34,  3.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.5321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 4400/6966 [24:42<14:52,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.5220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 4500/6966 [25:16<13:18,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.5124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 4600/6966 [25:50<12:36,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.5032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4700/6966 [26:24<12:22,  3.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.4945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 4800/6966 [26:58<12:26,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.4861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 4900/6966 [27:32<11:07,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.4780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 5000/6966 [28:06<10:59,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.4703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 5100/6966 [28:40<10:46,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.4629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 5200/6966 [29:14<09:56,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.4557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 5300/6966 [29:49<09:33,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.4488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 5400/6966 [30:23<09:07,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.4422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 5500/6966 [30:57<08:19,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.4358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 5600/6966 [31:32<08:11,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.4296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 5700/6966 [32:05<07:24,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.4237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5800/6966 [32:40<06:35,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.4179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 5900/6966 [33:13<05:56,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.4124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 6000/6966 [33:47<05:12,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.4070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 6100/6966 [34:21<04:59,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.4018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 6200/6966 [34:55<04:20,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.3968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 6300/6966 [35:29<03:46,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.3919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 6400/6966 [36:03<03:22,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.3872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 6500/6966 [36:37<02:33,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.3826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 6600/6966 [37:11<02:01,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.3781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 6700/6966 [37:44<01:29,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.3738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 6800/6966 [38:18<00:55,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.3696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 6900/6966 [38:52<00:22,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train loss: 0.3656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6966/6966 [39:13<00:00,  2.96it/s]\n",
      "100%|██████████| 871/871 [04:31<00:00,  3.21it/s]\n",
      "100%|██████████| 871/871 [04:30<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Train loss: 0.3630\n",
      "====> Epoch: 1 Validation loss: nan\n",
      "====> Epoch: 1 Test loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 100/6966 [00:33<37:10,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Train loss: 0.0874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 200/6966 [01:07<36:29,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Train loss: 0.0886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 300/6966 [01:40<37:21,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Train loss: 0.0883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 400/6966 [02:14<37:15,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Train loss: 0.0881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 500/6966 [02:47<33:55,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Train loss: 0.0883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 598/6966 [03:20<35:53,  2.96it/s]"
     ]
    }
   ],
   "source": [
    "loss_train, loss_test, loss_val = [], [], []\n",
    "#traing loop\n",
    "def train():\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    step = 0\n",
    "    for data in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        #copy data.x tensor to another tensor\n",
    "        orig_x = data.x.clone().detach()\n",
    "        #print(data.x.shape, orig_x.shape)\n",
    "        z, mu, sigma, mc1_loss, o1_loss, mc2_loss, o2_loss, mc3_loss, o3_loss = model(data)\n",
    "        #print(data.x.shape, orig_x.shape, z.shape)\n",
    "        loss = loss_function(z, orig_x, mu, sigma, mc1_loss, o1_loss, mc2_loss, o2_loss, mc3_loss, o3_loss)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        #print(loss.item())\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "        if step%100 == 0:\n",
    "            print(\"Epoch: {} Train loss: {:.4f}\".format(\n",
    "          epoch+1, train_loss/(step*16)))\n",
    "    return train_loss/len(train_loader.dataset)\n",
    "    \n",
    "    \n",
    "#validation loop\n",
    "def validation():\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for data in tqdm(val_loader):\n",
    "        data = data.to(device)\n",
    "        orig_x = data.x.clone().detach()\n",
    "        z, mu, sigma, mc1_loss, o1_loss, mc2_loss, o2_loss, mc3_loss, o3_loss = model(data)\n",
    "        loss = loss_function(z, orig_x, mu, sigma, mc1_loss, o1_loss, mc2_loss, o2_loss, mc3_loss, o3_loss)\n",
    "        val_loss += loss.item()\n",
    "    return val_loss/len(val_loader.dataset)\n",
    "\n",
    "    \n",
    "#validation loop\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for data in tqdm(test_loader):\n",
    "        data = data.to(device)\n",
    "        orig_x = data.x.clone().detach()\n",
    "        z, mu, sigma, mc1_loss, o1_loss, mc2_loss, o2_loss, mc3_loss, o3_loss = model(data)\n",
    "        loss = loss_function(z, orig_x, mu, sigma, mc1_loss, o1_loss, mc2_loss, o2_loss, mc3_loss, o3_loss)\n",
    "        test_loss += loss.item()\n",
    "    return test_loss/len(test_loader.dataset)\n",
    "\n",
    "#training loop\n",
    "for epoch in range(10):\n",
    "    train_loss = train()\n",
    "    val_loss = validation()\n",
    "    test_loss = test()\n",
    "    #scheduler.step()\n",
    "    print('====> Epoch: {} Train loss: {:.4f}'.format(\n",
    "          epoch+1, train_loss))\n",
    "    print('====> Epoch: {} Validation loss: {:.4f}'.format(\n",
    "          epoch+1, val_loss))\n",
    "    print('====> Epoch: {} Test loss: {:.4f}'.format(\n",
    "            epoch+1, test_loss))\n",
    "    loss_train.append(train_loss)\n",
    "    loss_test.append(test_loss)\n",
    "    loss_val.append(val_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_pip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
